{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. Install system dependencies first\n!sudo apt-get update\n!sudo apt-get install -y libvips\n\n# 2. Install PyTorch and its ecosystem for your specific CUDA version (cu121)\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n\n# 3. Install flash-attn AFTER PyTorch\n!pip install flash-attn\n\n# 4. Install the remaining Python packages (CORRECTED LINE)\n!pip install PyMuPDF weasyprint gradio transformers sentence-transformers faiss-cpu accelerate bitsandbytes Pillow pyvips","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:43:51.186964Z","iopub.execute_input":"2025-05-28T09:43:51.187495Z","iopub.status.idle":"2025-05-28T09:47:22.008829Z","shell.execute_reply.started":"2025-05-28T09:43:51.187473Z","shell.execute_reply":"2025-05-28T09:47:22.007919Z"}},"outputs":[{"name":"stdout","text":"Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\nGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \nHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease                         \nGet:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]           \nGet:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \nGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,702 kB]\nGet:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\nGet:9 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,728 kB]\nGet:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]     \nGet:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\nGet:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\nGet:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,968 kB] \nGet:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [34.3 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,264 kB]\nGet:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.6 kB]\nGet:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,402 kB]\nGet:19 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.1 kB]\nGet:20 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,951 kB]\nGet:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,564 kB]\nGet:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,552 kB]\nGet:23 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\nGet:24 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\nFetched 32.1 MB in 2s (13.1 MB/s)                             \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nNote, selecting 'libvips42' instead of 'libvips'\nThe following additional packages will be installed:\n  apparmor firefox libcgif0 libfuse3-3 libgail-common libgail18 libgsf-1-114\n  libgsf-1-common libgsl27 libgslcblas0 libgtk2.0-0 libgtk2.0-bin\n  libgtk2.0-common libimagequant0 libmatio11 libopenslide0 libpoppler-dev\n  libpoppler-glib8 libpoppler-private-dev libpoppler118 libudev1 nip2 snapd\n  squashfs-tools systemd-hwe-hwdb udev\nSuggested packages:\n  apparmor-profiles-extra apparmor-utils fuse3 gsl-ref-psdoc | gsl-doc-pdf\n  | gsl-doc-info | gsl-ref-html gvfs libvips-doc libvips-tools zenity\n  | kdialog\nThe following NEW packages will be installed:\n  apparmor firefox libcgif0 libfuse3-3 libgail-common libgail18 libgsf-1-114\n  libgsf-1-common libgsl27 libgslcblas0 libgtk2.0-0 libgtk2.0-bin\n  libgtk2.0-common libimagequant0 libmatio11 libopenslide0 libpoppler-glib8\n  libvips42 nip2 snapd squashfs-tools systemd-hwe-hwdb udev\nThe following packages will be upgraded:\n  libpoppler-dev libpoppler-private-dev libpoppler118 libudev1\n4 upgraded, 23 newly installed, 0 to remove and 154 not upgraded.\nNeed to get 41.7 MB of archives.\nAfter this operation, 154 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.15 [76.6 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.15 [1,557 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.67.1+22.04 [27.8 MB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 firefox amd64 1:1snap1-0ubuntu2 [72.3 kB]\nGet:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcgif0 amd64 0.2.0-1 [9,636 B]\nGet:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\nGet:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgsf-1-common all 1.14.47-1ubuntu0.1 [13.0 kB]\nGet:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgsf-1-114 amd64 1.14.47-1ubuntu0.1 [111 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgslcblas0 amd64 2.7.1+dfsg-3 [94.4 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgsl27 amd64 2.7.1+dfsg-3 [1,000 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\nGet:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libimagequant0 amd64 2.17.0-1 [34.6 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmatio11 amd64 1.5.21-1 [112 kB]\nGet:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenslide0 amd64 3.4.1+dfsg-5build1 [89.8 kB]\nGet:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-private-dev amd64 22.02.0-2ubuntu0.8 [199 kB]\nGet:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-dev amd64 22.02.0-2ubuntu0.8 [5,186 B]\nGet:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler118 amd64 22.02.0-2ubuntu0.8 [1,072 kB]\nGet:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-glib8 amd64 22.02.0-2ubuntu0.8 [134 kB]\nGet:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libvips42 amd64 8.12.1-1build1 [1,242 kB]\nGet:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 nip2 amd64 8.7.1-2build1 [4,893 kB]\nGet:27 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\nFetched 41.7 MB in 1s (45.2 MB/s)             \ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 27.)\ndebconf: falling back to frontend: Readline\nPreconfiguring packages ...\nSelecting previously unselected package apparmor.\n(Reading database ... 129184 files and directories currently installed.)\nPreparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\nUnpacking apparmor (3.0.4-2ubuntu2.4) ...\nSelecting previously unselected package squashfs-tools.\nPreparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\nUnpacking squashfs-tools (1:4.5-3build1) ...\nPreparing to unpack .../libudev1_249.11-0ubuntu3.15_amd64.deb ...\nUnpacking libudev1:amd64 (249.11-0ubuntu3.15) over (249.11-0ubuntu3.12) ...\nSetting up libudev1:amd64 (249.11-0ubuntu3.15) ...\nSelecting previously unselected package udev.\n(Reading database ... 129384 files and directories currently installed.)\nPreparing to unpack .../udev_249.11-0ubuntu3.15_amd64.deb ...\nUnpacking udev (249.11-0ubuntu3.15) ...\nSelecting previously unselected package libfuse3-3:amd64.\nPreparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\nUnpacking libfuse3-3:amd64 (3.10.5-1build1) ...\nSelecting previously unselected package snapd.\nPreparing to unpack .../snapd_2.67.1+22.04_amd64.deb ...\nUnpacking snapd (2.67.1+22.04) ...\nSetting up apparmor (3.0.4-2ubuntu2.4) ...\ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\ndebconf: falling back to frontend: Readline\nCreated symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\nSetting up squashfs-tools (1:4.5-3build1) ...\nSetting up udev (249.11-0ubuntu3.15) ...\ninvoke-rc.d: could not determine current runlevel\ninvoke-rc.d: policy-rc.d denied execution of start.\nSetting up libfuse3-3:amd64 (3.10.5-1build1) ...\nSetting up snapd (2.67.1+22.04) ...\nCreated symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\nCreated symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\nUnit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\nCreated symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\nCreated symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\nCreated symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\nCreated symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\nSelecting previously unselected package firefox.\n(Reading database ... 129613 files and directories currently installed.)\nPreparing to unpack .../00-firefox_1%3a1snap1-0ubuntu2_amd64.deb ...\ndebconf: unable to initialize frontend: Dialog\ndebconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\ndebconf: falling back to frontend: Readline\n=> Installing the firefox snap\n==> Checking connectivity with the snap store\n===> System doesn't have a working snapd, skipping\nUnpacking firefox (1:1snap1-0ubuntu2) ...\nSelecting previously unselected package libcgif0.\nPreparing to unpack .../01-libcgif0_0.2.0-1_amd64.deb ...\nUnpacking libcgif0 (0.2.0-1) ...\nSelecting previously unselected package libgtk2.0-common.\nPreparing to unpack .../02-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\nUnpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\nSelecting previously unselected package libgtk2.0-0:amd64.\nPreparing to unpack .../03-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\nUnpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\nSelecting previously unselected package libgail18:amd64.\nPreparing to unpack .../04-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\nUnpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\nSelecting previously unselected package libgail-common:amd64.\nPreparing to unpack .../05-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\nUnpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\nSelecting previously unselected package libgsf-1-common.\nPreparing to unpack .../06-libgsf-1-common_1.14.47-1ubuntu0.1_all.deb ...\nUnpacking libgsf-1-common (1.14.47-1ubuntu0.1) ...\nSelecting previously unselected package libgsf-1-114:amd64.\nPreparing to unpack .../07-libgsf-1-114_1.14.47-1ubuntu0.1_amd64.deb ...\nUnpacking libgsf-1-114:amd64 (1.14.47-1ubuntu0.1) ...\nSelecting previously unselected package libgslcblas0:amd64.\nPreparing to unpack .../08-libgslcblas0_2.7.1+dfsg-3_amd64.deb ...\nUnpacking libgslcblas0:amd64 (2.7.1+dfsg-3) ...\nSelecting previously unselected package libgsl27:amd64.\nPreparing to unpack .../09-libgsl27_2.7.1+dfsg-3_amd64.deb ...\nUnpacking libgsl27:amd64 (2.7.1+dfsg-3) ...\nSelecting previously unselected package libgtk2.0-bin.\nPreparing to unpack .../10-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\nUnpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\nSelecting previously unselected package libimagequant0:amd64.\nPreparing to unpack .../11-libimagequant0_2.17.0-1_amd64.deb ...\nUnpacking libimagequant0:amd64 (2.17.0-1) ...\nSelecting previously unselected package libmatio11:amd64.\nPreparing to unpack .../12-libmatio11_1.5.21-1_amd64.deb ...\nUnpacking libmatio11:amd64 (1.5.21-1) ...\nSelecting previously unselected package libopenslide0.\nPreparing to unpack .../13-libopenslide0_3.4.1+dfsg-5build1_amd64.deb ...\nUnpacking libopenslide0 (3.4.1+dfsg-5build1) ...\nPreparing to unpack .../14-libpoppler-private-dev_22.02.0-2ubuntu0.8_amd64.deb ...\nUnpacking libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.8) over (22.02.0-2ubuntu0.6) ...\nPreparing to unpack .../15-libpoppler-dev_22.02.0-2ubuntu0.8_amd64.deb ...\nUnpacking libpoppler-dev:amd64 (22.02.0-2ubuntu0.8) over (22.02.0-2ubuntu0.6) ...\nPreparing to unpack .../16-libpoppler118_22.02.0-2ubuntu0.8_amd64.deb ...\nUnpacking libpoppler118:amd64 (22.02.0-2ubuntu0.8) over (22.02.0-2ubuntu0.6) ...\nSelecting previously unselected package libpoppler-glib8:amd64.\nPreparing to unpack .../17-libpoppler-glib8_22.02.0-2ubuntu0.8_amd64.deb ...\nUnpacking libpoppler-glib8:amd64 (22.02.0-2ubuntu0.8) ...\nSelecting previously unselected package libvips42:amd64.\nPreparing to unpack .../18-libvips42_8.12.1-1build1_amd64.deb ...\nUnpacking libvips42:amd64 (8.12.1-1build1) ...\nSelecting previously unselected package nip2.\nPreparing to unpack .../19-nip2_8.7.1-2build1_amd64.deb ...\nUnpacking nip2 (8.7.1-2build1) ...\nSelecting previously unselected package systemd-hwe-hwdb.\nPreparing to unpack .../20-systemd-hwe-hwdb_249.11.5_all.deb ...\nUnpacking systemd-hwe-hwdb (249.11.5) ...\nSetting up libgsf-1-common (1.14.47-1ubuntu0.1) ...\nSetting up libcgif0 (0.2.0-1) ...\nSetting up firefox (1:1snap1-0ubuntu2) ...\nupdate-alternatives: using /usr/bin/firefox to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\nupdate-alternatives: using /usr/bin/firefox to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\nSetting up libgslcblas0:amd64 (2.7.1+dfsg-3) ...\nSetting up libgsl27:amd64 (2.7.1+dfsg-3) ...\nSetting up libgsf-1-114:amd64 (1.14.47-1ubuntu0.1) ...\nSetting up libpoppler118:amd64 (22.02.0-2ubuntu0.8) ...\nSetting up libimagequant0:amd64 (2.17.0-1) ...\nSetting up libmatio11:amd64 (1.5.21-1) ...\nSetting up systemd-hwe-hwdb (249.11.5) ...\nSetting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\nSetting up libopenslide0 (3.4.1+dfsg-5build1) ...\nSetting up libpoppler-dev:amd64 (22.02.0-2ubuntu0.8) ...\nSetting up libpoppler-glib8:amd64 (22.02.0-2ubuntu0.8) ...\nSetting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\nSetting up libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.8) ...\nSetting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\nSetting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\nSetting up libvips42:amd64 (8.12.1-1build1) ...\nSetting up nip2 (8.7.1-2build1) ...\nSetting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for dbus (1.12.20-2ubuntu4.1) ...\nProcessing triggers for shared-mime-info (2.1-2) ...\nProcessing triggers for udev (249.11-0ubuntu3.15) ...\nProcessing triggers for mailcap (3.70+nmu1ubuntu1) ...\nProcessing triggers for hicolor-icon-theme (0.17-2) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\nLooking in indexes: https://download.pytorch.org/whl/cu121\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nCollecting xformers\n  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hINFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==3.1.0 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.9.41)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hINFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl (15.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchaudio, xformers, torchvision\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvtx-cu12-12.1.105 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0 xformers-0.0.29.post1\nCollecting flash-attn\n  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.5.1+cu121)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.1.105)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.9.41)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\nBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187815463 sha256=d944fc7d2f962bce83fc4708c2fc0c21eaf8255962a0b350ae919362a51b7ef2\n  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\nSuccessfully built flash-attn\nInstalling collected packages: flash-attn\nSuccessfully installed flash-attn-2.7.4.post1\nCollecting PyMuPDF\n  Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\nCollecting weasyprint\n  Downloading weasyprint-65.1-py3-none-any.whl.metadata (3.7 kB)\nCollecting gradio\n  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nCollecting pyvips\n  Downloading pyvips-3.0.0.tar.gz (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting pydyf>=0.11.0 (from weasyprint)\n  Downloading pydyf-0.11.0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.11/dist-packages (from weasyprint) (1.17.1)\nCollecting tinyhtml5>=2.0.0b1 (from weasyprint)\n  Downloading tinyhtml5-2.0.0-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: tinycss2>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from weasyprint) (1.4.0)\nCollecting cssselect2>=0.8.0 (from weasyprint)\n  Downloading cssselect2-0.8.0-py3-none-any.whl.metadata (2.9 kB)\nCollecting Pyphen>=0.9.1 (from weasyprint)\n  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: fonttools>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from fonttools[woff]>=4.0.0->weasyprint) (4.57.0)\nRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.10.1 (from gradio)\n  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting groovy~=0.1 (from gradio)\n  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\nRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\nCollecting ruff>=0.9.3 (from gradio)\n  Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\nCollecting uvicorn>=0.14.0 (from gradio)\n  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=0.6->weasyprint) (2.22)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from cssselect2>=0.8.0->weasyprint) (0.5.1)\nCollecting brotli>=1.0.1 (from fonttools[woff]>=4.0.0->weasyprint)\n  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\nCollecting zopfli>=0.1.4 (from fonttools[woff]>=4.0.0->weasyprint)\n  Downloading zopfli-0.2.3.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.9.41)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading weasyprint-65.1-py3-none-any.whl (298 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cssselect2-0.8.0-py3-none-any.whl (15 kB)\nDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\nDownloading pydyf-0.11.0-py3-none-any.whl (8.1 kB)\nDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tinyhtml5-2.0.0-py3-none-any.whl (39 kB)\nDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading zopfli-0.2.3.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (850 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m850.6/850.6 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyvips\n  Building wheel for pyvips (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyvips: filename=pyvips-3.0.0-py3-none-any.whl size=54256 sha256=5b9c8545a156554fb39eff53b9ff94f8f1d051c3010dfd4d3efb685c0604d193\n  Stored in directory: /root/.cache/pip/wheels/8d/87/bb/ce9a0c257881486852c02c8c50a021684807b40d9579ec4568\nSuccessfully built pyvips\nInstalling collected packages: brotli, zopfli, uvicorn, tomlkit, tinyhtml5, semantic-version, ruff, python-multipart, Pyphen, PyMuPDF, pydyf, groovy, ffmpy, starlette, pyvips, cssselect2, weasyprint, safehttpx, gradio-client, fastapi, gradio, faiss-cpu, bitsandbytes\nSuccessfully installed PyMuPDF-1.26.0 Pyphen-0.17.2 bitsandbytes-0.46.0 brotli-1.1.0 cssselect2-0.8.0 faiss-cpu-1.11.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.31.0 gradio-client-1.10.1 groovy-0.1.2 pydyf-0.11.0 python-multipart-0.0.20 pyvips-3.0.0 ruff-0.11.11 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tinyhtml5-2.0.0 tomlkit-0.13.2 uvicorn-0.34.2 weasyprint-65.1 zopfli-0.2.3.post1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:47:22.744493Z","iopub.execute_input":"2025-05-28T09:47:22.744780Z","iopub.status.idle":"2025-05-28T09:47:23.147447Z","shell.execute_reply.started":"2025-05-28T09:47:22.744749Z","shell.execute_reply":"2025-05-28T09:47:23.146566Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"426c13c48d7745028a79ef103921f600"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nEnhanced PDF Processing Pipeline: RAG Chat & Summarization \nApplication\n======================================================================\nVersion 5: Dynamic Attention Mechanism & Robust Memory Management\nKey changes:\n- Automatically detects GPU compute capability to select the best\n  attention mechanism (Flash Attention 2 for Ampere+, SDPA for others).\n- Implemented VLM unloading to fix CUDA out-of-memory errors.\n- Activated gradient checkpointing for memory-efficient model operation.\n- Added more robust memory management with gc.collect() and torch.cuda.empty_cache().\n\"\"\"\n\nimport os\nimport re\nimport time\nimport json\nimport gc\nfrom datetime import datetime\nfrom typing import List, Dict, Tuple, Optional, Any\nfrom dataclasses import dataclass, field\nimport tempfile\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Core dependencies\nimport torch\nimport torch.nn as nn\nimport fitz  # PyMuPDF\nimport numpy as np\nfrom PIL import Image\nimport io\nimport markdown\nimport gradio as gr\n\ntry:\n    from weasyprint import HTML\n    WEASYPRINT_AVAILABLE = True\nexcept ImportError:\n    WEASYPRINT_AVAILABLE = False\n\nfrom tqdm import tqdm\n\n# AI/ML dependencies\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, AutoProcessor,\n    BitsAndBytesConfig, GenerationConfig\n)\nfrom sentence_transformers import SentenceTransformer\n\ntry:\n    import faiss\n    FAISS_AVAILABLE = True\n    print(\"✅ FAISS library found. RAG will use FAISS for vector indexing.\")\nexcept ImportError:\n    FAISS_AVAILABLE = False\n    print(\"⚠️ FAISS library not found. RAG functionality will be disabled.\")\n\ntry:\n    import bitsandbytes\n    BNB_AVAILABLE = True\n    print(\"✅ bitsandbytes library found. Will attempt 4-bit model loading.\")\nexcept ImportError:\n    BNB_AVAILABLE = False\n    print(\"⚠️ bitsandbytes library not found.\")\n\n# Kaggle/T4 optimizations\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nif torch.cuda.is_available():\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\n# ===========================================\n# CONFIGURATION\n# ===========================================\n\n@dataclass\nclass PipelineConfig:\n    \"\"\"Configuration for the PDF processing and RAG pipeline\"\"\"\n    # RAG chunking\n    rag_max_chars_per_chunk: int = 1500\n    rag_overlap_chars: int = 200\n\n    # Summarization chunking - adjusted for Gemma model\n    summarization_max_chars_per_chunk: int = 20000\n    summarization_overlap_chars: int = 2500\n\n    # Summarization LLM settings - updated for Gemma\n    map_summary_max_tokens: int = 800\n    reduce_summary_max_tokens: int = 3000\n\n    skip_vlm: bool = False\n    use_cache: bool = True\n\n    # GPU/Model optimizations\n    precision: str = \"auto\"\n    compile_model: bool = False  # Disabled for better compatibility\n    \n    # Diagnostic toggle\n    save_diagnostics: bool = True\n    \n    # Force single GPU usage for better performance\n    force_single_gpu: bool = True\n\n    # Updated LLM choice to Gemma\n    llm_models_priority: List[str] = field(default_factory=lambda: [\n        \"google/gemma-3-4b-it\"\n    ])\n    \n    embedding_model: str = \"NovaSearch/stella_en_400M_v5\"\n    moondream_model: str = \"vikhyatk/moondream2\"\n    moondream_revision: str = \"2025-04-14\"\n\n    # RAG settings\n    rag_top_k_chunks: int = 4\n\n    torch_dtype: Any = field(init=False)\n    attention_implementation: str = field(init=False)\n\n    def __post_init__(self):\n        # Set torch dtype\n        if self.precision == \"auto\":\n            self.torch_dtype = torch.float16\n        elif self.precision == \"fp16\":\n            self.torch_dtype = torch.float16\n        elif self.precision == \"bf16\" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n            self.torch_dtype = torch.bfloat16\n        else:\n            self.torch_dtype = torch.float32\n\n        # OPTIMIZATION: Auto-select attention mechanism based on GPU capability\n        if torch.cuda.is_available():\n            major, _ = torch.cuda.get_device_capability()\n            if major >= 8:\n                print(\"✅ GPU (Compute Capability >= 8.0) supports Flash Attention 2. Using 'flash_attention_2'.\")\n                self.attention_implementation = \"sdpa\"\n            else:\n                print(\"⚠️ GPU does not support Flash Attention 2. Falling back to 'sdpa' for optimization.\")\n                self.attention_implementation = \"sdpa\"\n        else:\n            print(\"ℹ️ No CUDA GPU found. Using default 'eager' attention mechanism.\")\n            self.attention_implementation = \"eager\"\n\n\n# ===========================================\n# GPU UTILITIES\n# ===========================================\ndef setup_gpu_environment():\n    if not torch.cuda.is_available():\n        print(\"⚠️ CUDA not available! Running on CPU.\")\n        return False\n    gpu_count = torch.cuda.device_count()\n    print(f\"🔍 Detected {gpu_count} GPU(s)\")\n    for i in range(gpu_count):\n        gpu_name = torch.cuda.get_device_name(i)\n        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n        major, minor = torch.cuda.get_device_capability(i)\n        print(f\"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB), Compute Capability: {major}.{minor}\")\n    return True\n\ndef get_device_map(model_name: str, force_single_gpu: bool = False) -> str | Dict:\n    gpu_count = torch.cuda.device_count()\n    if gpu_count == 0:\n        return \"cpu\"\n    if force_single_gpu or gpu_count == 1:\n        return {\"\": 0}  # Force everything to GPU 0\n    return \"auto\"\n\n# ===========================================\n# PERFORMANCE MONITORING\n# ===========================================\nclass PerformanceMonitor:\n    def __init__(self):\n        self.timings = {}\n        self.metrics = {}\n    \n    def start_timer(self, name: str):\n        self.timings[name] = time.time()\n    \n    def stop_timer(self, name: str) -> float:\n        elapsed = time.time() - self.timings.get(name, time.time())\n        self.metrics[f\"{name}_time_sec\"] = elapsed\n        print(f\"⏱️ {name} took: {elapsed:.2f}s\")\n        return elapsed\n    \n    def log_metric(self, name: str, value: Any):\n        self.metrics[name] = value\n        print(f\"📊 Metric {name}: {value}\")\n\n# ===========================================\n# DIAGNOSTIC LOGGER\n# ===========================================\nclass DiagnosticLogger:\n    def __init__(self, config: PipelineConfig):\n        self.config = config\n        self.log_dir = \"\"\n        if self.config.save_diagnostics:\n            self.log_dir = f\"diagnostics_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n            os.makedirs(self.log_dir, exist_ok=True)\n            print(f\"🔬 Diagnostics enabled. Saving logs to: {self.log_dir}\")\n\n    def log(self, filename: str, content: Any):\n        if not self.config.save_diagnostics or not self.log_dir:\n            return\n        filepath = os.path.join(self.log_dir, filename)\n        try:\n            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n                if isinstance(content, str):\n                    f.write(content)\n                else:\n                    json.dump(content, f, indent=4)\n            print(f\"📄 Diagnostic log saved: {filepath}\")\n        except Exception as e:\n            print(f\"⚠️ Failed to save diagnostic log {filename}: {e}\")\n\n    def log_performance(self, metrics: Dict[str, Any]):\n        self.log(\"performance_metrics.json\", metrics)\n\n# ===========================================\n# VLM - MOONDREAM (With Unloading Capability)\n# ===========================================\nclass MoondreamVLM:\n    def __init__(self, config: PipelineConfig, perf_monitor: Optional[PerformanceMonitor] = None):\n        self.config = config\n        self.perf_monitor = perf_monitor if perf_monitor else PerformanceMonitor()\n        self.model, self.processor = None, None\n        if self.config.skip_vlm:\n            print(\"⏭️ VLM (Moondream) is skipped by config.\")\n            return\n        self._load_vlm()\n\n    def _load_vlm(self):\n        self.perf_monitor.start_timer(\"load_vlm\")\n        print(f\"🌙 Loading VLM: {self.config.moondream_model}\")\n        try:\n            device_for_vlm = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n            self.processor = AutoTokenizer.from_pretrained(\n                self.config.moondream_model, \n                revision=self.config.moondream_revision\n            )\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.config.moondream_model, \n                revision=self.config.moondream_revision,\n                trust_remote_code=True, \n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            ).to(device_for_vlm)\n            self.model.eval()\n            print(f\"✅ VLM {self.config.moondream_model} loaded on {device_for_vlm}.\")\n        except Exception as e:\n            print(f\"❌ Failed to load VLM {self.config.moondream_model}: {e}\")\n            self.model, self.processor = None, None\n        self.perf_monitor.stop_timer(\"load_vlm\")\n\n    def caption_images_batch(self, images: List[Image.Image]) -> List[str]:\n        if not self.model or not self.processor or self.config.skip_vlm:\n            return [f\"Image (VLM skipped or not loaded)\" for _ in images]\n        self.perf_monitor.start_timer(\"vlm_captioning_batch\")\n        captions = []\n        for img in tqdm(images, desc=\"Captioning images with VLM\"):\n            try:\n                if img.mode != \"RGB\":\n                    img = img.convert(\"RGB\")\n                enc_image = self.model.encode_image(img)\n                caption_text = self.model.answer_question(\n                    image_embeds=enc_image, \n                    question=\"Describe the content of this image.\", \n                    tokenizer=self.processor\n                )\n                captions.append(caption_text if caption_text else \"Could not generate caption.\")\n            except Exception as e:\n                print(f\"⚠️ Error captioning image: {e}\")\n                captions.append(\"Error generating caption.\")\n        self.perf_monitor.stop_timer(\"vlm_captioning_batch\")\n        return captions\n\n    def unload_vlm(self):\n        \"\"\"Explicitly unload the VLM to free up GPU memory.\"\"\"\n        self.perf_monitor.start_timer(\"unload_vlm\")\n        if self.model:\n            print(\"🗑️ Unloading VLM to free up GPU memory...\")\n            del self.model\n            del self.processor\n            self.model, self.processor = None, None\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            print(\"✅ VLM unloaded and GPU memory cleared.\")\n        self.perf_monitor.stop_timer(\"unload_vlm\")\n\n# ===========================================\n# OPTIMIZED TEXT GENERATOR (LLM) - UPDATED FOR GEMMA\n# ===========================================\nclass OptimizedTextGenerator:\n    def __init__(self, config: PipelineConfig, perf_monitor: Optional[PerformanceMonitor] = None):\n        self.config = config\n        self.perf_monitor = perf_monitor if perf_monitor else PerformanceMonitor()\n        self.model, self.tokenizer, self.processor = None, None, None\n        self.is_gemma = False\n        self._load_llm()\n        \n    def _load_llm(self):\n        self.perf_monitor.start_timer(\"load_llm\")\n        print(\"🚀 Loading LLM (Gemma-3-4b-it)...\")\n        \n        quantization_config = None\n        if BNB_AVAILABLE:\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,\n                bnb_4bit_use_double_quant=True\n            )\n            print(\"Using BitsAndBytes 4-bit quantization config.\")\n\n        for model_id in self.config.llm_models_priority:\n            print(f\"Attempting to load LLM: {model_id}\")\n            \n            self.is_gemma = \"gemma\" in model_id.lower()\n            \n            try:\n                if self.is_gemma:\n                    self.processor = AutoProcessor.from_pretrained(model_id, padding_side=\"left\")\n                    self.tokenizer = self.processor.tokenizer if hasattr(self.processor, 'tokenizer') else self.processor\n                else:\n                    self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, use_fast=True)\n                    if self.tokenizer.pad_token is None:\n                        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n                model_kwargs = {\n                    \"trust_remote_code\": True,\n                    \"low_cpu_mem_usage\": True,\n                    \"device_map\": get_device_map(model_id, self.config.force_single_gpu),\n                    # OPTIMIZATION: Use the best available attention implementation\n                    \"attn_implementation\": self.config.attention_implementation,\n                }\n                \n                if quantization_config:\n                    model_kwargs[\"quantization_config\"] = quantization_config\n                    model_kwargs[\"torch_dtype\"] = torch.bfloat16\n                else:\n                    model_kwargs[\"torch_dtype\"] = self.config.torch_dtype\n\n                if self.is_gemma:\n                    try:\n                        from transformers import Gemma3ForConditionalGeneration\n                        model_class = Gemma3ForConditionalGeneration\n                    except ImportError:\n                        model_class = AutoModelForCausalLM\n\n                    self.model = model_class.from_pretrained(model_id, **model_kwargs)\n                    # OPTIMIZATION: Enable gradient checkpointing to save memory\n                    self.model.gradient_checkpointing_enable()\n                else:\n                    self.model = AutoModelForCausalLM.from_pretrained(model_id, **model_kwargs)\n                \n                print(f\"✅ LLM {model_id} loaded successfully.\")\n                self.model.eval()\n                \n                if hasattr(self.model, 'hf_device_map'):\n                    print(f\"📊 LLM distribution: {self.model.hf_device_map}\")\n                \n                self.perf_monitor.stop_timer(\"load_llm\")\n                return\n                \n            except Exception as e:\n                print(f\"❌ Failed to load LLM {model_id}: {e}\")\n                if hasattr(self, 'model') and self.model is not None:\n                    del self.model\n                    self.model = None\n                if hasattr(self, 'tokenizer') and self.tokenizer is not None:\n                    del self.tokenizer\n                    self.tokenizer = None\n                if hasattr(self, 'processor') and self.processor is not None:\n                    del self.processor\n                    self.processor = None\n                torch.cuda.empty_cache()\n                gc.collect()\n                \n        self.perf_monitor.stop_timer(\"load_llm\")\n        raise RuntimeError(\"Failed to load any specified LLM.\")\n\n    def generate_text(self, prompt: str, max_new_tokens: int = 512, system_prompt: str = None) -> str:\n        self.perf_monitor.start_timer(\"llm_generation\")\n        if not self.model or not (self.tokenizer or self.processor):\n            return \"LLM not loaded.\"\n\n        if self.is_gemma and self.processor:\n            messages = []\n            if system_prompt:\n                messages.append({\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]})\n            messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]})\n            \n            inputs = self.processor.apply_chat_template(\n                messages, \n                tokenize=True, \n                return_dict=True, \n                return_tensors=\"pt\", \n                add_generation_prompt=True\n            ).to(self.model.device)\n        else:\n            messages = [{\"role\": \"user\", \"content\": prompt}]\n            \n            if hasattr(self.tokenizer, 'apply_chat_template'):\n                text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n            else:\n                text = prompt\n\n            inputs = self.tokenizer(\n                text, \n                return_tensors=\"pt\", \n                truncation=True, \n                max_length=4096 - max_new_tokens\n            ).to(self.model.device)\n        \n        input_token_count = inputs.input_ids.shape[1]\n        \n        gen_kwargs = {\n            \"max_new_tokens\": max_new_tokens,\n            \"num_beams\": 1,\n            \"early_stopping\": True,\n            \"do_sample\": True,\n            \"temperature\": 0.7,\n            \"top_p\": 0.9,\n            \"repetition_penalty\": 1.1,\n        }\n\n        with torch.inference_mode():\n            outputs = self.model.generate(\n                inputs[\"input_ids\"],\n                attention_mask=inputs.get(\"attention_mask\"),\n                **gen_kwargs\n            )\n        \n        generated_ids = outputs[0][input_token_count:]\n        \n        if self.processor and hasattr(self.processor, 'decode'):\n            generated_text = self.processor.decode(generated_ids, skip_special_tokens=True).strip()\n        else:\n            generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n        \n        elapsed = time.time() - self.perf_monitor.timings.get(\"llm_generation\", time.time())\n        tokens_per_second = len(generated_ids) / elapsed if elapsed > 0 else 0\n        \n        self.perf_monitor.stop_timer(\"llm_generation\")\n        self.perf_monitor.log_metric(\"llm_output_tokens\", len(generated_ids))\n        self.perf_monitor.log_metric(\"tokens_per_second\", tokens_per_second)\n        \n        del inputs, outputs\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        return self._clean_output(generated_text)\n\n    def _clean_output(self, text: str) -> str:\n        text = re.sub(r'<\\|.*?\\|>', '', text)\n        text = text.replace(\"Answer:\", \"\").replace(\"Summary:\", \"\").strip()\n        return text.strip()\n\n# ===========================================\n# PDF PROCESSOR & CHUNKING (Unchanged)\n# ===========================================\n@dataclass\nclass ExtractedContent:\n    id: str\n    text: str\n    type: str\n    page_num: Optional[int] = None\n    metadata: Dict = field(default_factory=dict)\n\nclass PDFProcessor:\n    def __init__(self, config: PipelineConfig, perf_monitor: Optional[PerformanceMonitor] = None):\n        self.config = config\n        self.perf_monitor = perf_monitor if perf_monitor else PerformanceMonitor()\n\n    def extract_content_from_pdf(self, pdf_path: str) -> Tuple[List[Dict], List[Image.Image], List[int]]:\n        self.perf_monitor.start_timer(\"pdf_raw_extraction\")\n        print(f\"📄 Extracting raw content from PDF: {pdf_path}\")\n        raw_text_by_page, pil_images, image_page_numbers = [], [], []\n        try:\n            doc = fitz.open(pdf_path)\n        except Exception as e:\n            print(f\"❌ Error opening PDF {pdf_path}: {e}\")\n            return [], [], []\n        \n        for page_num_idx in tqdm(range(len(doc)), desc=\"Extracting pages\"):\n            page = doc[page_num_idx]\n            page_num_actual = page_num_idx + 1\n            text = page.get_text(\"text\")\n            if text.strip():\n                raw_text_by_page.append({'page_num': page_num_actual, 'text': text})\n            \n            if not self.config.skip_vlm:\n                for img_index, img_info in enumerate(page.get_images(full=True)):\n                    xref = img_info[0]\n                    try:\n                        base_image = doc.extract_image(xref)\n                        image_bytes = base_image[\"image\"]\n                        pil_image = Image.open(io.BytesIO(image_bytes))\n                        if pil_image.size[0] > 1024 or pil_image.size[1] > 1024:\n                            pil_image.thumbnail((1024, 1024), Image.Resampling.LANCZOS)\n                        pil_images.append(pil_image)\n                        image_page_numbers.append(page_num_actual)\n                    except Exception as e_img:\n                        print(f\"⚠️ Could not extract image xref {xref} on page {page_num_actual}: {e_img}\")\n        \n        doc.close()\n        self.perf_monitor.stop_timer(\"pdf_raw_extraction\")\n        self.perf_monitor.log_metric(\"num_raw_images_extracted\", len(pil_images))\n        print(f\"✅ Raw content extraction complete. {len(raw_text_by_page)} text pages, {len(pil_images)} images.\")\n        return raw_text_by_page, pil_images, image_page_numbers\n\n    def chunk_for_rag(self, raw_text_by_page: List[Dict], image_captions_with_pages: List[Dict]) -> List[ExtractedContent]:\n        self.perf_monitor.start_timer(\"rag_chunking\")\n        all_rag_content = []\n        chunk_id_counter = 0\n        \n        full_doc_text_annotated = \"\".join([f\"[Page {item['page_num']}]\\n{item['text']}\\n\\n\" for item in raw_text_by_page])\n        text_segments = self._chunk_text_strategy(full_doc_text_annotated, self.config.rag_max_chars_per_chunk, self.config.rag_overlap_chars)\n        \n        for i, segment in enumerate(text_segments):\n            page_nums_in_segment = set(re.findall(r\"\\[Page (\\d+)\\]\", segment))\n            first_page = min(map(int, page_nums_in_segment)) if page_nums_in_segment else None\n            clean_segment = re.sub(r\"\\[Page \\d+\\]\\n?\", \"\", segment).strip()\n            if not clean_segment:\n                continue\n            all_rag_content.append(ExtractedContent(\n                id=f\"text_chunk_{chunk_id_counter}\",\n                text=clean_segment,\n                type=\"text_chunk\",\n                page_num=first_page,\n                metadata={'source': 'text'}\n            ))\n            chunk_id_counter += 1\n        \n        for i, cap_info in enumerate(image_captions_with_pages):\n            caption_text = f\"Image on page {cap_info['page_num']}: {cap_info['caption']}\"\n            all_rag_content.append(ExtractedContent(\n                id=f\"img_cap_{chunk_id_counter}\",\n                text=caption_text,\n                type=\"image_caption\",\n                page_num=cap_info['page_num'],\n                metadata={'source': 'image_caption'}\n            ))\n            chunk_id_counter += 1\n        \n        self.perf_monitor.stop_timer(\"rag_chunking\")\n        self.perf_monitor.log_metric(\"num_rag_chunks\", len(all_rag_content))\n        return all_rag_content\n\n    def chunk_for_summarization(self, raw_text_by_page: List[Dict], image_captions_with_pages: List[Dict]) -> List[str]:\n        self.perf_monitor.start_timer(\"summarization_chunking\")\n        page_texts_map = {item['page_num']: item['text'] for item in raw_text_by_page}\n        page_captions_map = {}\n        for cap_info in image_captions_with_pages:\n            p_num = cap_info['page_num']\n            if p_num not in page_captions_map:\n                page_captions_map[p_num] = []\n            page_captions_map[p_num].append(f\"(Image: {cap_info['caption']})\")\n        \n        full_text_for_summary = \"\"\n        for p_num in sorted(list(set(page_texts_map.keys()) | set(page_captions_map.keys()))):\n            full_text_for_summary += f\"[Page {p_num}]\\n\"\n            if p_num in page_texts_map:\n                full_text_for_summary += page_texts_map[p_num] + \"\\n\"\n            if p_num in page_captions_map:\n                full_text_for_summary += \" \".join(page_captions_map[p_num]) + \"\\n\"\n            full_text_for_summary += \"\\n\"\n        \n        summarization_chunks = self._chunk_text_strategy(\n            full_text_for_summary, \n            self.config.summarization_max_chars_per_chunk, \n            self.config.summarization_overlap_chars\n        )\n        self.perf_monitor.stop_timer(\"summarization_chunking\")\n        self.perf_monitor.log_metric(\"num_summarization_chunks\", len(summarization_chunks))\n        return summarization_chunks\n\n    def _chunk_text_strategy(self, text: str, max_chars: int, overlap: int) -> List[str]:\n        if not text:\n            return []\n        chunks = []\n        start_idx = 0\n        text_len = len(text)\n        while start_idx < text_len:\n            end_idx = min(start_idx + max_chars, text_len)\n            chunks.append(text[start_idx:end_idx])\n            if end_idx == text_len:\n                break\n            start_idx += (max_chars - overlap)\n            if start_idx >= end_idx:\n                start_idx = end_idx\n        return chunks\n\n# ===========================================\n# EMBEDDING AND VECTOR STORE (Unchanged)\n# ===========================================\nclass RAGEmbeddingStore:\n    def __init__(self, config: PipelineConfig, perf_monitor: Optional[PerformanceMonitor] = None):\n        self.config = config\n        self.perf_monitor = perf_monitor if perf_monitor else PerformanceMonitor()\n        self.embedding_model, self.index, self.content_map = None, None, []\n        if not FAISS_AVAILABLE:\n            print(\"⚠️ FAISS not available. RAG Store cannot be initialized.\")\n            return\n        self._load_embedding_model()\n\n    def _load_embedding_model(self):\n        self.perf_monitor.start_timer(\"load_embedding_model\")\n        try:\n            print(f\"🌟 Loading embedding model: {self.config.embedding_model}\")\n            self.embedding_model = SentenceTransformer(\n                self.config.embedding_model,\n                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n                trust_remote_code=True\n            )\n            print(f\"✅ Embedding model '{self.config.embedding_model}' loaded.\")\n        except Exception as e:\n            print(f\"❌ Failed to load embedding model: {e}\")\n            self.embedding_model = None\n        self.perf_monitor.stop_timer(\"load_embedding_model\")\n\n    def build_index(self, all_content: List[ExtractedContent]):\n        if not self.embedding_model or not FAISS_AVAILABLE:\n            print(\"⚠️ Cannot build index.\")\n            return False\n        if not all_content:\n            print(\"⚠️ No content to build index from.\")\n            return False\n        \n        self.perf_monitor.start_timer(\"build_faiss_index\")\n        self.content_map = all_content\n        content_texts = [content.text for content in all_content]\n        print(f\"⏳ Generating embeddings for {len(content_texts)} content pieces...\")\n        embeddings = self.embedding_model.encode(content_texts, show_progress_bar=True, batch_size=32)\n        embeddings = np.array(embeddings).astype('float32')\n        embedding_dim = embeddings.shape[1]\n        self.index = faiss.IndexFlatIP(embedding_dim)\n        self.index.add(embeddings)\n        self.perf_monitor.stop_timer(\"build_faiss_index\")\n        self.perf_monitor.log_metric(\"faiss_index_size\", self.index.ntotal if self.index else 0)\n        print(f\"✅ FAISS index built with {self.index.ntotal if self.index else 0} vectors.\")\n        return True\n\n    def retrieve_relevant_content(self, query: str, top_k: int) -> List[ExtractedContent]:\n        if not self.index or not self.embedding_model or not self.content_map:\n            return []\n        self.perf_monitor.start_timer(\"retrieve_content\")\n        query_embedding = self.embedding_model.encode([query], prompt_name=\"s2p_query\")\n        query_embedding = np.array(query_embedding).astype('float32')\n        distances, indices = self.index.search(query_embedding, top_k)\n        retrieved = [self.content_map[idx] for idx in indices[0] if 0 <= idx < len(self.content_map)]\n        self.perf_monitor.stop_timer(\"retrieve_content\")\n        return retrieved\n\n# ===========================================\n# GRADIO APPLICATION STATE & LOGIC\n# ===========================================\nAPP_CONFIG = PipelineConfig(save_diagnostics=True, force_single_gpu=True)\nDIAGNOSTIC_LOGGER = DiagnosticLogger(APP_CONFIG)\n\nPDF_PROCESSOR_INSTANCE = PDFProcessor(APP_CONFIG)\nLLM_GENERATOR_INSTANCE, VLM_CAPTIONER_INSTANCE, EMBEDDING_STORE_INSTANCE = None, None, None\nSESSION_RAW_TEXT_BY_PAGE, SESSION_PIL_IMAGES, SESSION_IMAGE_PAGE_NUMBERS, SESSION_IMAGE_CAPTIONS_WITH_PAGES = [], [], [], []\n\ndef initialize_all_models_for_gradio():\n    global LLM_GENERATOR_INSTANCE, VLM_CAPTIONER_INSTANCE, EMBEDDING_STORE_INSTANCE\n    if LLM_GENERATOR_INSTANCE is None:\n        print(\"Initializing LLM...\")\n        LLM_GENERATOR_INSTANCE = OptimizedTextGenerator(APP_CONFIG)\n    if not APP_CONFIG.skip_vlm and VLM_CAPTIONER_INSTANCE is None:\n        print(\"Initializing VLM...\")\n        VLM_CAPTIONER_INSTANCE = MoondreamVLM(APP_CONFIG)\n    if EMBEDDING_STORE_INSTANCE is None and FAISS_AVAILABLE:\n        print(\"Initializing Embedding Store...\")\n        EMBEDDING_STORE_INSTANCE = RAGEmbeddingStore(APP_CONFIG)\n\ndef process_uploaded_pdf_for_app(pdf_file_obj, progress=gr.Progress(track_tqdm=True)):\n    global SESSION_RAW_TEXT_BY_PAGE, SESSION_PIL_IMAGES, SESSION_IMAGE_PAGE_NUMBERS, SESSION_IMAGE_CAPTIONS_WITH_PAGES\n    if pdf_file_obj is None:\n        return None, \"Please upload a PDF file.\", False, False, []\n    \n    initialize_all_models_for_gradio()\n    pdf_path = pdf_file_obj.name\n    pdf_basename = os.path.basename(pdf_path)\n    status_updates = [f\"Processing '{pdf_basename}'...\"]\n    progress(0.1, desc=status_updates[-1])\n\n    local_raw_text, local_pil_images, local_image_page_numbers = PDF_PROCESSOR_INSTANCE.extract_content_from_pdf(pdf_path)\n\n    SESSION_RAW_TEXT_BY_PAGE = local_raw_text\n    SESSION_PIL_IMAGES = local_pil_images\n    SESSION_IMAGE_PAGE_NUMBERS = local_image_page_numbers\n\n    status_updates.append(f\"Extracted {len(local_raw_text)} text pages and {len(local_pil_images)} images.\")\n    progress(0.3, desc=status_updates[-1])\n\n    if not local_raw_text and not local_pil_images:\n        return pdf_basename, \"\\n\".join(status_updates) + \"\\nNo content extracted.\", False, False, []\n\n    local_image_captions_with_pages = []\n    if not APP_CONFIG.skip_vlm and VLM_CAPTIONER_INSTANCE and local_pil_images:\n        status_updates.append(\"Generating image captions...\")\n        progress(0.5, desc=status_updates[-1])\n        \n        captions = VLM_CAPTIONER_INSTANCE.caption_images_batch(local_pil_images)\n        \n        for i, caption_text in enumerate(captions):\n            local_image_captions_with_pages.append({\n                'page_num': local_image_page_numbers[i], \n                'caption': caption_text, \n                'image_index_on_page': i\n            })\n            \n        status_updates.append(f\"Generated {len(local_image_captions_with_pages)} image captions.\")\n        DIAGNOSTIC_LOGGER.log(\"1_image_captions.json\", local_image_captions_with_pages)\n        \n        # OPTIMIZATION: Unload VLM to free up memory before summarization\n        VLM_CAPTIONER_INSTANCE.unload_vlm()\n\n    elif APP_CONFIG.skip_vlm:\n        status_updates.append(\"Image captioning (VLM) is skipped by configuration.\")\n\n    SESSION_IMAGE_CAPTIONS_WITH_PAGES = local_image_captions_with_pages\n    progress(0.7, desc=status_updates[-1])\n\n    rag_ready = False\n    if EMBEDDING_STORE_INSTANCE and EMBEDDING_STORE_INSTANCE.embedding_model:\n        rag_content_pieces = PDF_PROCESSOR_INSTANCE.chunk_for_rag(local_raw_text, local_image_captions_with_pages)\n        if rag_content_pieces:\n            status_updates.append(\"Building RAG search index...\")\n            progress(0.8, desc=status_updates[-1])\n            rag_ready = EMBEDDING_STORE_INSTANCE.build_index(rag_content_pieces)\n            if rag_ready:\n                status_updates.append(\"RAG index built. Ready for chat.\")\n            else:\n                status_updates.append(\"Failed to build RAG index.\")\n        else:\n            status_updates.append(\"No content to build RAG index from.\")\n    else:\n        status_updates.append(\"RAG store/embedding model not available. Chat will be limited.\")\n\n    final_status = \"\\n\".join(status_updates)\n    progress(1.0, desc=\"Processing complete.\")\n    summary_ready = bool(local_raw_text)\n    \n    if LLM_GENERATOR_INSTANCE:\n        DIAGNOSTIC_LOGGER.log_performance(LLM_GENERATOR_INSTANCE.perf_monitor.metrics)\n\n    return pdf_basename, final_status, rag_ready, summary_ready, []\n\ndef handle_rag_chat_response(message: str, chat_history: List[Tuple[str,str]], processed_pdf_name: Optional[str], rag_ready_state: bool):\n    if not rag_ready_state or processed_pdf_name is None:\n        chat_history.append((message, \"Error: PDF not processed for RAG or RAG not ready.\"))\n        return \"\", chat_history\n    if LLM_GENERATOR_INSTANCE is None or EMBEDDING_STORE_INSTANCE is None:\n        chat_history.append((message, \"Error: LLM or Embedding store not initialized.\"))\n        return \"\", chat_history\n    \n    retrieved_content = EMBEDDING_STORE_INSTANCE.retrieve_relevant_content(message, top_k=APP_CONFIG.rag_top_k_chunks)\n    if not retrieved_content:\n        response_text = \"I couldn't find relevant information in the document to answer your question.\"\n    else:\n        context_parts = []\n        source_pages = set()\n        for content_item in retrieved_content:\n            context_parts.append(content_item.text)\n            if content_item.page_num:\n                source_pages.add(content_item.page_num)\n        context_str_for_prompt = \"\\n\\n---\\n\\n\".join(context_parts)\n        prompt = f\"\"\"Based on the following context from the document '{processed_pdf_name}' ONLY, answer the question. If the answer is not found in the context, clearly state that.\n\nContext:\n{context_str_for_prompt}\n\nQuestion: {message}\n\nAnswer:\"\"\"\n        \n        rag_interaction_data = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"question\": message,\n            \"retrieved_context\": context_str_for_prompt\n        }\n        timestamp = datetime.now().strftime('%H%M%S')\n        DIAGNOSTIC_LOGGER.log(f\"rag_interaction_{timestamp}.json\", rag_interaction_data)\n        \n        response_text = LLM_GENERATOR_INSTANCE.generate_text(prompt, max_new_tokens=512)\n        if source_pages:\n            response_text += f\"\\n\\n(Sources: Approx. Page(s) {', '.join(map(str, sorted(list(source_pages))))})\"\n    \n    chat_history.append((message, response_text))\n    return \"\", chat_history\n\ndef handle_summarize_document(processed_pdf_name: Optional[str], summary_ready_state: bool, progress=gr.Progress(track_tqdm=True)):\n    if not summary_ready_state or processed_pdf_name is None:\n        return \"Error: PDF not processed or no text content available for summarization.\"\n    if LLM_GENERATOR_INSTANCE is None:\n        return \"Error: LLM not initialized for summarization.\"\n    if not SESSION_RAW_TEXT_BY_PAGE:\n        return \"No text content was extracted from the PDF to summarize.\"\n    \n    status_updates = [f\"Starting summarization for '{processed_pdf_name}'...\"]\n    progress(0.1, desc=status_updates[-1])\n\n    summarization_chunks = PDF_PROCESSOR_INSTANCE.chunk_for_summarization(SESSION_RAW_TEXT_BY_PAGE, SESSION_IMAGE_CAPTIONS_WITH_PAGES)\n    DIAGNOSTIC_LOGGER.log(\"2a_summarization_chunks.json\", summarization_chunks)\n\n    if not summarization_chunks:\n        return \"Could not prepare any chunks for summarization.\"\n    \n    status_updates.append(f\"Created {len(summarization_chunks)} large chunks for map-reduce summarization.\")\n    progress(0.2, desc=status_updates[-1])\n\n    chunk_summaries_with_source = []\n    \n    for i, chunk_text in enumerate(summarization_chunks):\n        map_progress = 0.2 + (0.6 * (i + 1) / len(summarization_chunks))\n        progress(map_progress, desc=f\"Summarizing chunk {i+1}/{len(summarization_chunks)}...\")\n        \n        prompt = f\"\"\"Please generate a comprehensive and detailed summary of the following text segment. This summary should capture ALL the key points, main arguments and any significant conclusions presented *within this segment only*. Maintain a neutral, objective, and informative tone. The target length for this segment's summary is approximately {(APP_CONFIG.map_summary_max_tokens//4) - 30} words. Do not add any introductory or concluding phrases that are not part of the summary content itself (e.g., avoid 'Here is the summary:').\n\nTEXT SEGMENT TO SUMMARIZE:\n-------------------------\n{chunk_text[:APP_CONFIG.summarization_max_chars_per_chunk]}\n-------------------------\nBegin the summary directly.\"\"\"\n\n        system_prompt = \"You are a helpful assistant specialized in summarizing document segments.\"\n        \n        summary = LLM_GENERATOR_INSTANCE.generate_text(\n            prompt, \n            max_new_tokens=APP_CONFIG.map_summary_max_tokens + 200,\n            system_prompt=system_prompt\n        )\n        \n        chunk_summaries_with_source.append({\n            \"chunk_index\": i,\n            \"chunk_text\": chunk_text,\n            \"summary\": summary\n        })\n        status_updates.append(f\"Summary for chunk {i+1} generated.\")\n\n    DIAGNOSTIC_LOGGER.log(\"2b_chunk_summaries.json\", chunk_summaries_with_source)\n    chunk_summaries = [item['summary'] for item in chunk_summaries_with_source]\n\n    if not chunk_summaries:\n        return \"Failed to generate summaries for any chunk.\"\n    \n    status_updates.append(\"Combining chunk summaries into a final document summary...\")\n    progress(0.9, desc=status_updates[-1])\n\n    combined_chunk_summaries_text = \"\\n\\n\".join(chunk_summaries)\n    DIAGNOSTIC_LOGGER.log(\"2c_combined_chunk_summaries.txt\", combined_chunk_summaries_text)\n\n    if len(combined_chunk_summaries_text) > 40000:\n        final_summary = f\"# Summary of {processed_pdf_name}\\n\\n\"\n        final_summary += \"The document is extensive. Here are the key points from each section:\\n\\n\"\n        for i, summary in enumerate(chunk_summaries):\n            final_summary += f\"## Section {i+1}\\n\\n{summary}\\n\\n\"\n    else:\n        system_prompt_overall = (\n            \"You are an expert AI assistant tasked with synthesizing multiple summaries of document segments into a single, comprehensive, and coherent final summary. \"\n            \"Your primary objective is to integrate the information from the provided segment summaries, eliminate redundancy, and produce a well-structured final document that accurately reflects the core content of the original source, based *only* on the summaries provided.\"\n        )\n        \n        user_prompt_overall = f\"\"\"You have been provided with a collection of sequential summaries, where each summary covers a distinct segment of a larger document. Your task is to synthesize these individual segment summaries into one cohesive and comprehensive final summary of the original document. The final summary must be based entirely on the information present in the segment summaries provided below.\n\n**Input: Collection of Segment Summaries:**\n---------------------------------------\n{combined_chunk_summaries_text}\n---------------------------------------\n\n**Output Requirements for the Final Consolidated Summary:**\n1.  **Content Focus:** Integrate ALL the information, key themes, arguments, and conclusions from the provided segment summaries. Identify and remove any redundancies or overlapping points.\n2.  **Structure & Formatting:** The output MUST be in well-structured Markdown format and include the following distinct sections:\n    a.  **Main Title:** A suitable H1 or H2 Markdown heading for the overall summary (e.g., `# Comprehensive Summary of [Document Topic]`).\n    b.  **Introductory Paragraph:** A brief introduction that outlines the main topic and overall scope of the original document, as can be inferred from the collective summaries.\n    c.  **Key Terms and Descriptions Section:** A dedicated section with a subheading (e.g., `## Key Terms and Concepts`) that lists significant key terms or concepts encountered across all the segment summaries. Each term should be followed by a brief, clear description based on its context in the summaries.\n    d.  **Main Body:** The core of the summary, presenting a logical and flowing narrative of the document's content by weaving together the information from the segment summaries. Ensure smooth transitions between topics derived from different segments.\n    e.  **Concluding Paragraph:** A final paragraph that wraps up the main points and provides a sense of closure, reflecting the overall essence captured in the segment summaries.\n3.  **No Extraneous Text:** The output MUST ONLY be the summary itself, structured as described above. Do NOT include any conversational text, meta-comments, or remarks outside of the summary content (e.g., avoid phrases like 'Here is the consolidated summary:' or 'I have followed your instructions.').\n4.  **Target Length:** Aim for a total output length of approximately {(APP_CONFIG.reduce_summary_max_tokens // 4) - 50} words for this final summary or about 5-6 pages.\n\nBegin the final consolidated summary directly with the Main Title.\"\"\"\n\n        final_summary = LLM_GENERATOR_INSTANCE.generate_text(\n            user_prompt_overall, \n            max_new_tokens=APP_CONFIG.reduce_summary_max_tokens + 200,\n            system_prompt=system_prompt_overall\n        )\n\n    DIAGNOSTIC_LOGGER.log(\"2d_final_summary.txt\", final_summary)\n\n    status_updates.append(\"Final summary generated.\")\n    progress(1.0, desc=\"Summarization complete.\")\n\n    if LLM_GENERATOR_INSTANCE:\n        DIAGNOSTIC_LOGGER.log_performance(LLM_GENERATOR_INSTANCE.perf_monitor.metrics)\n\n    return final_summary\n\ndef create_gradio_interface():\n    print(\"Creating Gradio interface...\")\n    setup_gpu_environment()\n    with gr.Blocks(theme=gr.themes.Glass()) as demo:\n        gr.Markdown(\"# 📄 PDF Assistant: Chat (RAG) & Summarize\")\n        gr.Markdown(\"Upload a PDF to chat about its content or get a comprehensive summary.\")\n        processed_pdf_name_state = gr.State(None)\n        rag_ready_state = gr.State(False)\n        summary_ready_state = gr.State(False)\n        \n        with gr.Row():\n            with gr.Column(scale=1):\n                gr.Markdown(\"### 1. Upload & Process PDF\")\n                pdf_upload = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n                process_button = gr.Button(\"Process PDF for Chat & Summary\", variant=\"primary\")\n                status_display = gr.Markdown(\"Status: Waiting for PDF...\")\n                gr.Markdown(\"---\")\n                gr.Markdown(\"### About This Assistant\")\n                gr.Markdown(f\"\"\"- **LLMs Used**: {', '.join(APP_CONFIG.llm_models_priority)} (4-bit quantized)\n- **Embeddings**: {APP_CONFIG.embedding_model}\n- **Vector Store**: FAISS (for RAG)\n- **Image Understanding**: {APP_CONFIG.moondream_model if not APP_CONFIG.skip_vlm else 'Disabled'}\n- **Diagnostics**: {'Enabled' if APP_CONFIG.save_diagnostics else 'Disabled'}\n- **GPU Usage**: {'Single GPU mode' if APP_CONFIG.force_single_gpu else 'Multi-GPU mode'}\"\"\")\n                if not FAISS_AVAILABLE:\n                    gr.Markdown(\"⚠️ **FAISS library not found. RAG chat functionality is disabled.**\")\n                if not BNB_AVAILABLE:\n                    gr.Markdown(\"⚠️ **BitsAndBytes not found. 4-bit models might fail to load.**\")\n            \n            with gr.Column(scale=2):\n                with gr.Tabs():\n                    with gr.TabItem(\"💬 Chat with PDF (RAG)\"):\n                        chatbot = gr.Chatbot(label=\"Chat History\", height=550)\n                        msg_textbox = gr.Textbox(label=\"Your Question:\", placeholder=\"Ask about text or images in the PDF...\", lines=2)\n                        with gr.Row():\n                            submit_chat_button = gr.Button(\"Send Question\", variant=\"primary\", elem_id=\"send_button_rag\")\n                            clear_chat_button = gr.Button(\"Clear Chat\")\n                    \n                    with gr.TabItem(\"📜 Summarize Document\"):\n                        summarize_button = gr.Button(\"Generate Full Document Summary\", variant=\"primary\")\n                        summary_output_display = gr.Markdown(label=\"Document Summary\", value=\"Summary will appear here...\")\n        \n        process_button.click(\n            fn=process_uploaded_pdf_for_app,\n            inputs=[pdf_upload],\n            outputs=[processed_pdf_name_state, status_display, rag_ready_state, summary_ready_state, chatbot]\n        )\n        msg_textbox.submit(\n            fn=handle_rag_chat_response,\n            inputs=[msg_textbox, chatbot, processed_pdf_name_state, rag_ready_state],\n            outputs=[msg_textbox, chatbot]\n        )\n        submit_chat_button.click(\n            fn=handle_rag_chat_response,\n            inputs=[msg_textbox, chatbot, processed_pdf_name_state, rag_ready_state],\n            outputs=[msg_textbox, chatbot]\n        )\n        clear_chat_button.click(lambda: (None, []), outputs=[msg_textbox, chatbot])\n        summarize_button.click(\n            fn=handle_summarize_document,\n            inputs=[processed_pdf_name_state, summary_ready_state],\n            outputs=[summary_output_display]\n        )\n    \n    return demo\n\n# ===========================================\n# MAIN EXECUTION\n# ===========================================\nif __name__ == \"__main__\":\n    gradio_app = create_gradio_interface()\n    print(\"Launching Gradio app...\")\n    gradio_app.launch(server_name=\"0.0.0.0\", server_port=7980, share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-28T09:54:23.835534Z","iopub.execute_input":"2025-05-28T09:54:23.836173Z","iopub.status.idle":"2025-05-28T09:54:25.381640Z","shell.execute_reply.started":"2025-05-28T09:54:23.836127Z","shell.execute_reply":"2025-05-28T09:54:25.380911Z"}},"outputs":[{"name":"stdout","text":"✅ FAISS library found. RAG will use FAISS for vector indexing.\n✅ bitsandbytes library found. Will attempt 4-bit model loading.\n⚠️ GPU does not support Flash Attention 2. Falling back to 'sdpa' for optimization.\n🔬 Diagnostics enabled. Saving logs to: diagnostics_20250528_095423\nCreating Gradio interface...\n🔍 Detected 2 GPU(s)\n  GPU 0: Tesla T4 (15.8 GB), Compute Capability: 7.5\n  GPU 1: Tesla T4 (15.8 GB), Compute Capability: 7.5\nLaunching Gradio app...\n* Running on local URL:  http://0.0.0.0:8796\n* Running on public URL: https://f920a4199275121600.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://f920a4199275121600.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Initializing LLM...\n🚀 Loading LLM (Gemma-3-4b-it)...\nUsing BitsAndBytes 4-bit quantization config.\nAttempting to load LLM: google/gemma-3-4b-it\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5bf4d9d9fc240c4a271c0b45165a08f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5c2572365994b23bc35ded7a903b3e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdd38ff2374249f0982beda2298808ee"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def2af9c5fd642b7b7e408875d112bd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6555e70a4f9c485fbbf2b4b6585db4ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92708bb4bb9e4573b1d40769746b32cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f722c98da527496084c1919407357e34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"312a18e028a8478fb889038e36ef4048"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b3f1d58ed5c4a6281013059b13fcd1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb8465de16a747b292def467c0df5dd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?steps/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb75e94fcc744d069a5610f404458688"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"648ce22a096c44daaa730631fa00bcac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8da3ef07790c4550aa71997c70703d9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf838d15626042f28b747478b0dbd06c"}},"metadata":{}},{"name":"stdout","text":"✅ LLM google/gemma-3-4b-it loaded successfully.\n📊 LLM distribution: {'': 0}\n⏱️ load_llm took: 45.71s\nInitializing VLM...\n🌙 Loading VLM: vikhyatk/moondream2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bf905ac91354c43816f51c0eb98bdff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55cb209d06924f81b9f6da22fa11829b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"650d1d97c956496380b8503dbefebfe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5994303bc274c4ea9ab27087a7593f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67b661970d034635a06bdbe9f1f8de44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c5c9d26081343318a485d153f14ffc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/276 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"449500fd59c846d0aaf863e50893c00c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"hf_moondream.py:   0%|          | 0.00/3.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b111aa61f7f94fb9bd8ff48e717f5dc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.py:   0%|          | 0.00/2.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"807f6784f0bd4d558adb88a8f3b1d5a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"region.py:   0%|          | 0.00/3.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f677b1ca0934c2aa6c6e1df2530baf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"layers.py:   0%|          | 0.00/1.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85dc463bfe004585864a8bf6259cc7c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"image_crops.py:   0%|          | 0.00/7.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde57f17cfa64166abdbded1abc78e9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"moondream.py:   0%|          | 0.00/26.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69862e54ff3b44ec9dc4e925f1507b54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"utils.py:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cad727c09bf943529432e68e0b46c681"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vision.py:   0%|          | 0.00/5.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"266165f296ed410e90b732afd3fb78cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text.py:   0%|          | 0.00/6.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06c8d1cf76e140dcbb0457b15e57706a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rope.py:   0%|          | 0.00/1.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49dc1abd5e1e470492894a69bb364309"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.85G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"211093f4697c4796b3f9c2288948e0ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/69.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf9b4c6e86d849a4b8206f151a01e02c"}},"metadata":{}},{"name":"stdout","text":"✅ VLM vikhyatk/moondream2 loaded on cuda:0.\n⏱️ load_vlm took: 23.43s\nInitializing Embedding Store...\n🌟 Loading embedding model: NovaSearch/stella_en_400M_v5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d45c94a80dfe46cb84186dce33a24da3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/397 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6598ae9a349b41d99a8ec35273a32ecd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/170k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc0af9523c4248e7b242f61437005402"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a1d859758814d4090c834c01176d520"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/892 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eec71b6670f48e0a7f3e9f4ed946054"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration.py:   0%|          | 0.00/7.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5fde4221f5c421dae2dd4eb443430d5"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/NovaSearch/stella_en_400M_v5:\n- configuration.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling.py:   0%|          | 0.00/57.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56f27945e2db4ac0acc9546b9214a588"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/NovaSearch/stella_en_400M_v5:\n- modeling.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48a098c169b749499ca0272a6e2174b5"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at NovaSearch/stella_en_400M_v5 were not used when initializing NewModel: ['pooler.dense.bias', 'pooler.dense.weight']\n- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcb5df5f8d2046ad9f5b7a5293e925ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49818ea449104026bf0b3150e0a8f560"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"335320f092af437899d2f364b3eeedc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d102cea134674c0d8a139acad5d2169b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/186 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a357cde93fb43379a55f9f2cde74f24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9df7d143ac6478097a964777bd87199"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/4.20M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a3d64e830c240e4ad046d2f330debff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.20M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbaa4fa8691d43d98fbb463bd27ddc24"}},"metadata":{}},{"name":"stdout","text":"✅ Embedding model 'NovaSearch/stella_en_400M_v5' loaded.\n⏱️ load_embedding_model took: 14.14s\n📄 Extracting raw content from PDF: /tmp/gradio/13d4aa47db180586efbbf79f3808ad8c92cb2618c220d44a6bca7323a30ad4ed/170609_student.pdf\n⏱️ pdf_raw_extraction took: 0.65s\n📊 Metric num_raw_images_extracted: 2\n✅ Raw content extraction complete. 67 text pages, 2 images.\n⏱️ vlm_captioning_batch took: 6.72s\n📄 Diagnostic log saved: diagnostics_20250528_095423/1_image_captions.json\n🗑️ Unloading VLM to free up GPU memory...\n✅ VLM unloaded and GPU memory cleared.\n⏱️ unload_vlm took: 0.60s\n⏱️ rag_chunking took: 0.00s\n📊 Metric num_rag_chunks: 141\n⏳ Generating embeddings for 141 content pieces...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/5 [00:00<?, ?steps/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d94030c2c615424399a1489b7f583a7f"}},"metadata":{}},{"name":"stdout","text":"⏱️ build_faiss_index took: 10.06s\n📊 Metric faiss_index_size: 141\n✅ FAISS index built with 141 vectors.\n📄 Diagnostic log saved: diagnostics_20250528_095423/performance_metrics.json\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?steps/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56f9923430d34f09a7ac8bcc4a8717f7"}},"metadata":{}},{"name":"stdout","text":"⏱️ retrieve_content took: 0.13s\n📄 Diagnostic log saved: diagnostics_20250528_095423/rag_interaction_100604.json\n⏱️ llm_generation took: 19.72s\n📊 Metric llm_output_tokens: 129\n📊 Metric tokens_per_second: 6.541860862056994\n⏱️ summarization_chunking took: 0.00s\n📊 Metric num_summarization_chunks: 11\n📄 Diagnostic log saved: diagnostics_20250528_095423/2a_summarization_chunks.json\n⏱️ llm_generation took: 59.61s\n📊 Metric llm_output_tokens: 344\n📊 Metric tokens_per_second: 5.770483482447557\n⏱️ llm_generation took: 57.77s\n📊 Metric llm_output_tokens: 334\n📊 Metric tokens_per_second: 5.781604927156025\n⏱️ llm_generation took: 52.94s\n📊 Metric llm_output_tokens: 299\n📊 Metric tokens_per_second: 5.647557783449537\n⏱️ llm_generation took: 59.54s\n📊 Metric llm_output_tokens: 355\n📊 Metric tokens_per_second: 5.96267139360413\n⏱️ llm_generation took: 54.75s\n📊 Metric llm_output_tokens: 307\n📊 Metric tokens_per_second: 5.607529443004869\n⏱️ llm_generation took: 54.84s\n📊 Metric llm_output_tokens: 315\n📊 Metric tokens_per_second: 5.743639232028876\n⏱️ llm_generation took: 69.73s\n📊 Metric llm_output_tokens: 412\n📊 Metric tokens_per_second: 5.908518229123259\n⏱️ llm_generation took: 52.17s\n📊 Metric llm_output_tokens: 285\n📊 Metric tokens_per_second: 5.46275005942582\n⏱️ llm_generation took: 58.51s\n📊 Metric llm_output_tokens: 327\n📊 Metric tokens_per_second: 5.588912792673682\n⏱️ llm_generation took: 56.10s\n📊 Metric llm_output_tokens: 324\n📊 Metric tokens_per_second: 5.7752476048207155\n⏱️ llm_generation took: 40.80s\n📊 Metric llm_output_tokens: 300\n📊 Metric tokens_per_second: 7.353082035531769\n📄 Diagnostic log saved: diagnostics_20250528_095423/2b_chunk_summaries.json\n📄 Diagnostic log saved: diagnostics_20250528_095423/2c_combined_chunk_summaries.txt\n⏱️ llm_generation took: 171.51s\n📊 Metric llm_output_tokens: 1354\n📊 Metric tokens_per_second: 7.894661127955738\n📄 Diagnostic log saved: diagnostics_20250528_095423/2d_final_summary.txt\n📄 Diagnostic log saved: diagnostics_20250528_095423/performance_metrics.json\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}